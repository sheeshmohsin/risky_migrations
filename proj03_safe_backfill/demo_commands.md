# Demo Commands for proj03_safe_backfill

This file contains step-by-step commands to demonstrate safe index creation using PostgreSQL's CREATE INDEX CONCURRENTLY.

## Setup (One-Time)

```bash
cd proj03_safe_backfill
docker compose up -d --build
docker compose exec web python manage.py seed_large_table --large-count 10000000 --batch-size 10000
```

## Current State

- Migration: `0001_initial` (tables created, no indexes on email field)
- Data: 10 rows in small_email, 10M rows in large_email
- Port: 8001 (vs proj03_indexes on 8000)
- Database: PostgreSQL 16

---

## Step 1: Verify Current State

**Check migrations:**
```bash
docker compose exec web python manage.py showmigrations demoapp
```

Expected output:
```
demoapp
 [X] 0001_initial
 [ ] 0002_add_email_index_concurrently
```

**Test API (should work fast):**
```bash
curl http://127.0.0.1:8001/list_large/
```

**Check row counts:**
```bash
docker compose exec db psql -U postgres -d risky_migrations -c "SELECT 'small_email' as table, COUNT(*) FROM small_email UNION ALL SELECT 'large_email', COUNT(*) FROM large_email;"
```

Expected output:
```
    table     |  count
--------------+----------
 small_email  |       10
 large_email  | 10000000
```

**Check table structure (no indexes yet):**
```bash
docker compose exec db psql -U postgres -d risky_migrations -c "\d large_email"
```

Expected output:
```
                                Table "public.large_email"
 Column |          Type          | Collation | Nullable |             Default
--------+------------------------+-----------+----------+----------------------------------
 id     | bigint                 |           | not null | generated by default as identity
 name   | character varying(100) |           | not null |
 email  | character varying(255) |           | not null |
Indexes:
    "large_email_pkey" PRIMARY KEY, btree (id)
```

Note: No index on `email` field yet!

---

## Step 2: Demonstrate Safe Index Creation with CONCURRENTLY

### Terminal 1: Monitor API Continuously

Run this in a loop to demonstrate that API remains responsive:
```bash
while true; do
  echo -n "$(date +%H:%M:%S) - "
  curl --max-time 3 http://127.0.0.1:8001/list_large/ 2>&1 | grep -q "emails" && echo "✓ API responded" || echo "✗ API blocked/timeout"
  sleep 1
done
```

### Terminal 2: Run the CONCURRENTLY Migration

```bash
docker compose exec web python manage.py migrate demoapp 0002
```

### What You'll Observe:

- **Terminal 2:** Migration takes time (building index on 10M rows)
- **Terminal 1:** API requests **CONTINUE WORKING** - no timeouts!
- **Key difference:** Zero downtime, no table locks

**Output from migration:**
```
Operations to perform:
  Target specific migration: 0002_add_email_index_concurrently, from demoapp
Running migrations:
  Applying demoapp.0002_add_email_index_concurrently... OK
```

---

## Step 3: Verify Index Was Created

**Check migration status:**
```bash
docker compose exec web python manage.py showmigrations demoapp
```

All migrations should now be applied:
```
demoapp
 [X] 0001_initial
 [X] 0002_add_email_index_concurrently
```

**Check table structure (index exists now):**
```bash
docker compose exec db psql -U postgres -d risky_migrations -c "\d large_email"
```

Expected output:
```
                                Table "public.large_email"
 Column |          Type          | Collation | Nullable |             Default
--------+------------------------+-----------+----------+----------------------------------
 id     | bigint                 |           | not null | generated by default as identity
 name   | character varying(100) |           | not null |
 email  | character varying(255) |           | not null |
Indexes:
    "large_email_pkey" PRIMARY KEY, btree (id)
    "large_email_email_idx" btree (email)  <-- NEW INDEX!
```

**Test API (still works):**
```bash
curl http://127.0.0.1:8001/list_large/
```

---

## Step 4: Understand How It Works

**View the migration file:**
```bash
docker compose exec web cat demoapp/migrations/0002_add_email_index_concurrently.py
```

**Key components:**
```python
from django.contrib.postgres.operations import AddIndexConcurrently

class Migration(migrations.Migration):
    atomic = False  # Required: CONCURRENTLY can't run in a transaction

    operations = [
        AddIndexConcurrently(
            model_name='largeemail',
            index=models.Index(fields=['email'], name='large_email_email_idx'),
        ),
    ]
```

**Why it's safe:**

1. **`atomic = False`**: Disables transaction wrapping (required for CONCURRENTLY)
2. **`AddIndexConcurrently`**: PostgreSQL-specific operation
3. **Creates index in multiple phases**: Scans table without holding exclusive locks
4. **Allows concurrent reads/writes**: Application traffic continues uninterrupted

---

## Step 5: Compare with Risky Approach (proj03_indexes)

### Side-by-Side Comparison

| Aspect | proj03_indexes (Risky) | proj03_safe_backfill (Safe) |
|--------|------------------------|----------------------------|
| **Port** | 8000 | 8001 |
| **Index Method** | Standard CREATE INDEX | CREATE INDEX CONCURRENTLY |
| **Blocks Reads?** | ✅ Yes | ❌ No |
| **Blocks Writes?** | ✅ Yes | ❌ No |
| **Downtime** | Minutes (10M+ rows) | None |
| **Migration Time** | Faster | Slower (more phases) |
| **Production Safe?** | ❌ No | ✅ Yes |

### To demonstrate the difference (optional):

**Terminal 1: Start proj03_indexes (risky version)**
```bash
cd ../proj03_indexes
docker compose up -d
# Seed data if not already done
curl http://127.0.0.1:8000/list_large/
```

**Terminal 2: Run risky migration**
```bash
cd ../proj03_indexes
docker compose exec web python manage.py migrate demoapp 0001  # rollback first
docker compose exec web python manage.py migrate demoapp 0002  # apply index
```

**Terminal 3: Try to query during migration**
```bash
curl http://127.0.0.1:8000/list_large/  # Will HANG/BLOCK!
```

---

## Step 6: Rollback and Re-run (Optional)

**Rollback to state before index:**
```bash
docker compose exec web python manage.py migrate demoapp 0001
```

**Verify index was removed:**
```bash
docker compose exec db psql -U postgres -d risky_migrations -c "\d large_email"
```

**Re-apply CONCURRENTLY migration:**
```bash
docker compose exec web python manage.py migrate demoapp 0002
```

**With Terminal 1 still monitoring**, you'll see zero downtime again!

---

## Advanced: Monitor PostgreSQL During Migration

### Check Active Queries and Locks

**Terminal 1: Monitor active connections**
```bash
watch -n 1 'docker compose exec db psql -U postgres -d risky_migrations -c "SELECT pid, usename, state, query, wait_event_type FROM pg_stat_activity WHERE datname = '\''risky_migrations'\'' AND query NOT LIKE '\''%pg_stat_activity%'\'';"'
```

**Terminal 2: Run migration**
```bash
docker compose exec web python manage.py migrate demoapp 0002
```

**What you'll see:**
- Migration runs its own query
- Other queries continue executing (not blocked)
- No `Lock` wait events for application queries

### Check Index Build Progress (PostgreSQL 12+)

```bash
docker compose exec db psql -U postgres -d risky_migrations -c "SELECT * FROM pg_stat_progress_create_index;"
```

---

## Performance Testing

### Test Query Speed Without Index

**Rollback to remove index:**
```bash
docker compose exec web python manage.py migrate demoapp 0001
```

**Test query performance:**
```bash
docker compose exec db psql -U postgres -d risky_migrations -c "EXPLAIN ANALYZE SELECT * FROM large_email WHERE email = 'user1000000@example.com';"
```

You'll see a **Seq Scan** (sequential scan) - slow!

### Test Query Speed With Index

**Re-apply index:**
```bash
docker compose exec web python manage.py migrate demoapp 0002
```

**Test query performance:**
```bash
docker compose exec db psql -U postgres -d risky_migrations -c "EXPLAIN ANALYZE SELECT * FROM large_email WHERE email = 'user1000000@example.com';"
```

You'll see an **Index Scan** using `large_email_email_idx` - much faster!

---

## Cleanup

**Stop containers:**
```bash
docker compose down
```

**Stop and remove data (deletes database):**
```bash
docker compose down -v
```

**Restart fresh:**
```bash
docker compose up -d --build
docker compose exec web python manage.py seed_large_table --large-count 10000000 --batch-size 10000
```

---

## Key Takeaways

### ❌ Wrong Way (Standard Index - proj03_indexes)
- Uses Django's default `db_index=True`
- Holds `ACCESS EXCLUSIVE` lock during creation
- Blocks ALL reads and writes
- Causes production downtime (minutes to hours on large tables)

### ✅ Right Way (CONCURRENTLY - this project)
- Uses `AddIndexConcurrently` operation
- Sets `atomic = False` in migration
- Creates index without blocking queries
- Zero downtime
- Takes longer but acceptable tradeoff

### Migration Code Comparison

**Risky (don't use on large tables):**
```python
operations = [
    migrations.AlterField(
        model_name='largeemail',
        name='email',
        field=models.CharField(max_length=255, db_index=True),  # BLOCKS!
    ),
]
```

**Safe (use this):**
```python
from django.contrib.postgres.operations import AddIndexConcurrently

class Migration(migrations.Migration):
    atomic = False  # Required!

    operations = [
        AddIndexConcurrently(
            model_name='largeemail',
            index=models.Index(fields=['email'], name='large_email_email_idx'),
        ),
    ]
```

---

## When to Use This Approach

✅ **Use CONCURRENTLY when:**
- Adding indexes to production tables with 100k+ rows
- Deploying during business hours
- Zero downtime is required
- Using PostgreSQL (9.2+)

❌ **Standard index is fine when:**
- Table has < 100k rows
- Deploying during maintenance window
- In development/testing environments
- Using SQLite (CONCURRENTLY not supported)

---

## Related Projects

- **proj03_indexes** - Demonstrates the risky standard index creation that blocks queries
- **proj01_safe_backfill** - Similar safe approach for adding NOT NULL constraints
- See the main `CLAUDE.md` for all 10 risky migration scenarios

---

## Troubleshooting

### "CONCURRENTLY cannot be executed from a function" error
- Make sure `atomic = False` is set in the migration class
- CONCURRENTLY cannot run inside a transaction

### Index creation fails
- Check PostgreSQL logs: `docker compose logs db`
- Ensure enough disk space and memory
- Try with fewer rows first

### API still blocks during migration
- Verify you're using `AddIndexConcurrently` (not regular `AddIndex`)
- Check you're testing against the right port (8001)
- Verify PostgreSQL version supports CONCURRENTLY (9.2+)
